---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: default
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prometheus
    role: alert-rules
  name: prometheus-example-rules
spec:
  groups:
  - name: DemoAlerts
    # Note: all the rules below need to include the endpoint, namespace, and job
    # labels to ensure the EDA Pod receives the firing events. Maybe this can be
    # configured/filtered using some filtering logic on the alertmanager YAMLs?
    rules:
    - alert: InstanceDown 
      # If the replica count of the deployment drops below the given threshold,
      # scale the replica count up.
      expr: count by(job, endpoint, service, container, namespace) (up{job="quarkus-monitor"}) < 2
      for: 10s
    - alert: MemoryUsage
      # Sum the simulated "memory usage" from all replicas, and scale the replica
      # count of the Deployment if it exceeds the defined threshold
      expr: sum by (job,namespace,endpoint,service,container) (current_memory) > 40
      for: 10s
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  alerting:
    alertmanagers:
    - namespace: default
      name: alertmanager-example
      port: web
  serviceMonitorSelector:
    matchLabels:
      team: backend
  ruleSelector:
    matchLabels:
      role: alert-rules
      prometheus: prometheus
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: true
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
spec:
  type: NodePort
  ports:
  - name: web
    nodePort: 30900
    port: 9090
    protocol: TCP
    targetPort: web
  selector:
    prometheus: prometheus
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    app: quarkus-monitor
  name: quarkus-monitor
spec:
  port:
    targetPort: 8080
  to:
    kind: Service
    name: quarkus-monitor
    weight: 100